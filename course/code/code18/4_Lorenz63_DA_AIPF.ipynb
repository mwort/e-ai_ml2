{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75598e30-bce5-407d-8a2a-90de53e32fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) FUNCTIONS: Lorenz-63 in torch, observation helpers, neural update (DeepSets-style)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Lorenz-63 in torch -----\n",
    "class Lorenz63Params:\n",
    "    def __init__(self, sigma=10.0, rho=28.0, beta=8.0/3.0):\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.beta = beta\n",
    "\n",
    "def lorenz63_rhs(x, p: Lorenz63Params):\n",
    "    if x.ndim == 1:  # single state (3,)\n",
    "        dx = p.sigma * (x[1] - x[0])\n",
    "        dy = x[0] * (p.rho - x[2]) - x[1]\n",
    "        dz = x[0] * x[1] - p.beta * x[2]\n",
    "        return torch.tensor([dx, dy, dz])\n",
    "    else:            # batch of states (N,3)\n",
    "        dx = p.sigma * (x[:,1] - x[:,0])\n",
    "        dy = x[:,0] * (p.rho - x[:,2]) - x[:,1]\n",
    "        dz = x[:,0] * x[:,1] - p.beta * x[:,2]\n",
    "        return torch.stack([dx, dy, dz], dim=1)\n",
    "\n",
    "\n",
    "def rk4_step(x, dt, f, params):\n",
    "    k1 = f(x, params)\n",
    "    k2 = f(x + 0.5*dt*k1, params)\n",
    "    k3 = f(x + 0.5*dt*k2, params)\n",
    "    k4 = f(x + dt*k3, params)\n",
    "    return x + (dt/6.0)*(k1 + 2*k2 + 2*k3 + k4)\n",
    "\n",
    "def integrate_l63(x0, params, dt, steps):\n",
    "    X = [x0]\n",
    "    x = x0\n",
    "    for _ in range(steps):\n",
    "        x = rk4_step(x, dt, lorenz63_rhs, params)\n",
    "        X.append(x)\n",
    "    return torch.stack(X)  # (steps+1,3)\n",
    "\n",
    "# ----- Observation helpers -----\n",
    "def make_obs_operator(select=\"xy\"):\n",
    "    if select == \"xy\":\n",
    "        H = torch.tensor([[1.,0.,0.],[0.,1.,0.]])\n",
    "    elif select == \"full\":\n",
    "        H = torch.eye(3)\n",
    "    else:\n",
    "        H = torch.tensor([[1.,0.,0.],[0.,0.,1.]])  # x and z\n",
    "    return H\n",
    "\n",
    "def add_obs_noise(H, X, R_std, rng=torch):\n",
    "    \"\"\"\n",
    "    X: (T,3), H:(m,3)\n",
    "    returns Y: (T,m)\n",
    "    \"\"\"\n",
    "    m = H.shape[0]\n",
    "    Y = (H @ X.T).T\n",
    "    noise = R_std * rng.randn(*Y.shape)\n",
    "    return Y + noise\n",
    "\n",
    "# ----- DeepSets neural update -----\n",
    "class ParticleUpdateNN(nn.Module):\n",
    "    def __init__(self, d_state=3, d_obs=2, hidden=64):\n",
    "        super().__init__()\n",
    "        # phi maps particle+obs -> embedding\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(d_state+d_obs, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # rho maps pooled embedding -> global context\n",
    "        self.rho = nn.Sequential(\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # psi maps (particle+context+obs) -> particle increment\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Linear(d_state+hidden+d_obs, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, d_state)\n",
    "        )\n",
    "    def forward(self, Xb, y):\n",
    "        \"\"\"\n",
    "        Xb: (N,d_state), y:(d_obs,)\n",
    "        returns Xa: (N,d_state)\n",
    "        \"\"\"\n",
    "        N = Xb.shape[0]\n",
    "        y_rep = y.expand(N,-1)\n",
    "        inp = torch.cat([Xb,y_rep], dim=1)  # (N,d_state+d_obs)\n",
    "        emb = self.phi(inp)                 # (N,hidden)\n",
    "        pooled = emb.mean(dim=0, keepdim=True)  # (1,hidden)\n",
    "        context = self.rho(pooled).expand(N,-1) # (N,hidden)\n",
    "        out_inp = torch.cat([Xb,context,y_rep], dim=1)\n",
    "        dX = self.psi(out_inp)\n",
    "        Xa = Xb + dX\n",
    "        return Xa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02b38dd-41f3-44da-b960-a09ddb82940d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth: torch.Size([151, 3])  Obs: torch.Size([151, 2])\n"
     ]
    }
   ],
   "source": [
    "# (2) TRUTH & OBSERVATIONS\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "params_true = Lorenz63Params()\n",
    "dt_assim = 0.1\n",
    "Nt = 150\n",
    "\n",
    "# truth trajectory\n",
    "x0_true = torch.tensor([1.0,1.0,20.0])\n",
    "X_truth = integrate_l63(x0_true, params_true, dt_assim, Nt)  # (Nt+1,3)\n",
    "\n",
    "# obs operator: observe x,y\n",
    "H = make_obs_operator(\"xy\")\n",
    "R_std = 0.1\n",
    "Y = add_obs_noise(H, X_truth, R_std)\n",
    "\n",
    "print(\"Truth:\", X_truth.shape, \" Obs:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981c871-3167-4d30-8f7a-6bf45cd3638a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, avg loss 1274.497\n",
      "Epoch  50, avg loss 1.228\n"
     ]
    }
   ],
   "source": [
    "# (3) TRAINING LOOP — background via Lorenz-63 forecast with sigma=11\n",
    "\n",
    "# model and optimizer\n",
    "d_state = 3; d_obs = H.shape[0]\n",
    "net = ParticleUpdateNN(d_state, d_obs, hidden=64)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "def obs_likelihood(y, Xa, H, R_std):\n",
    "    \"\"\"\n",
    "    y: (m,), Xa: (N,d)  -> returns loglik per particle (N,)\n",
    "    \"\"\"\n",
    "    Yp = (H @ Xa.T).T\n",
    "    innov = y - Yp\n",
    "    quad = (innov**2).sum(dim=1) / (R_std**2)\n",
    "    const = torch.tensor(2*torch.pi*(R_std**2), dtype=Xa.dtype, device=Xa.device)\n",
    "    loglik = -0.5 * (quad + y.shape[0] * torch.log(const))\n",
    "    return loglik\n",
    "\n",
    "# ensemble size and forecast settings\n",
    "Npart = 20\n",
    "nsub = 5                 # substeps per assimilation interval (numerical accuracy)\n",
    "model_std = 0.20         # optional additive model noise after forecast (0.0..0.5)\n",
    "\n",
    "# approximate/wrong model parameters for forecast\n",
    "params_model = Lorenz63Params(sigma=11.0, rho=28.0, beta=8/3)\n",
    "\n",
    "# initial analysis ensemble (biased start, like Gaussian PF)\n",
    "x0_bias = X_truth[0] + torch.tensor([2.0, -2.0, 3.0], dtype=X_truth.dtype)\n",
    "Xa_prev = x0_bias.expand(Npart, 3) + 2.0 * torch.randn(Npart, 3, dtype=X_truth.dtype)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(501):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # reset ensemble at start of each epoch (optional; comment out if you prefer continuity across epochs)\n",
    "    Xa_prev = x0_bias.expand(Npart, 3) + 2.0 * torch.randn(Npart, 3, dtype=X_truth.dtype)\n",
    "\n",
    "    for n in range(Nt):\n",
    "        # ---- Forecast: propagate Xa_prev -> Xb with wrong model (sigma=11)\n",
    "        Xb = Xa_prev\n",
    "        h = dt_assim / nsub\n",
    "        for _ in range(nsub):\n",
    "            Xb = rk4_step(Xb, h, lorenz63_rhs, params_model)\n",
    "\n",
    "        # optional additive model noise to maintain spread\n",
    "        if model_std > 0:\n",
    "            Xb = Xb + model_std * torch.randn_like(Xb)\n",
    "\n",
    "        # ---- Analysis via neural update\n",
    "        y = Y[n]                     # observation at time n\n",
    "        Xa = net(Xb, y)              # updated particles\n",
    "\n",
    "        # ---- Likelihood-based loss (no Xa target)\n",
    "        loglik = obs_likelihood(y, Xa, H, R_std)\n",
    "        # negative log average likelihood\n",
    "        loss = -(torch.logsumexp(loglik, dim=0) - torch.log(torch.tensor(Npart, dtype=X_truth.dtype)))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # detach analysis for next cycle to avoid backprop through time\n",
    "        Xa_prev = Xa.detach()\n",
    "\n",
    "    losses.append(total_loss / Nt)\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d}, avg loss {losses[-1]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b1624-0ca8-45c2-b6a6-6642b960ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) EVALUATION & VISUALIZATION: forecast-based xb, NN-based xa, errors\n",
    "# Uses Npart, params_model, nsub, model_std, dt_assim, net, X_truth, Y from above.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "Xb_means = []\n",
    "Xa_means = []\n",
    "\n",
    "# initial analysis ensemble (same style as training)\n",
    "device = X_truth.device\n",
    "dtype  = X_truth.dtype\n",
    "x0_bias = X_truth[0] + torch.tensor([2.0, -2.0, 3.0], device=device, dtype=dtype)\n",
    "Xa_prev = x0_bias.expand(Npart, 3) + 2.0 * torch.randn(Npart, 3, device=device, dtype=dtype)\n",
    "\n",
    "for n in range(Nt):\n",
    "    # ---- Forecast: Xa_prev -> Xb with wrong model (sigma=11)\n",
    "    Xb = Xa_prev\n",
    "    h = dt_assim / nsub\n",
    "    for _ in range(nsub):\n",
    "        Xb = rk4_step(Xb, h, lorenz63_rhs, params_model)\n",
    "\n",
    "    if model_std > 0:\n",
    "        Xb = Xb + model_std * torch.randn_like(Xb)\n",
    "\n",
    "    Xb_means.append(Xb.mean(dim=0))\n",
    "\n",
    "    # ---- Analysis via neural update\n",
    "    y = Y[n]                # observation at time n\n",
    "    Xa = net(Xb, y)\n",
    "    #Xa = Xb\n",
    "    Xa_means.append(Xa.mean(dim=0))\n",
    "\n",
    "    # next cycle\n",
    "    Xa_prev = Xa.detach()\n",
    "\n",
    "Xb_means = torch.stack(Xb_means)   # (Nt,3)\n",
    "Xa_means = torch.stack(Xa_means)   # (Nt,3)\n",
    "Xt = X_truth[:-1]                  # (Nt,3)\n",
    "\n",
    "# ---- component time series ----\n",
    "labels = [r\"$x_1$\", r\"$x_2$\", r\"$x_3$\"]\n",
    "fig, axs = plt.subplots(3, 1, figsize=(9, 8), sharex=True)\n",
    "for i in range(3):\n",
    "    axs[i].plot(Xt[:, i].detach().cpu().numpy(), 'k-', label=\"truth\")\n",
    "    axs[i].plot(Xb_means[:, i].detach().cpu().numpy(), 'r--', label=\"background\")\n",
    "    axs[i].plot(Xa_means[:, i].detach().cpu().numpy(), 'b-', label=\"analysis\")\n",
    "    axs[i].set_ylabel(labels[i])\n",
    "    axs[i].grid(alpha=0.3)\n",
    "    axs[i].legend(loc=\"upper right\")\n",
    "axs[-1].set_xlabel(\"time step\")\n",
    "fig.suptitle(\"Neural PF (torch): Truth vs Background vs Analysis (means)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lorenz63_aipf.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---- error norms over time ----\n",
    "err_b = torch.norm(Xb_means - Xt, dim=1)   # ||xb - xt||\n",
    "err_a = torch.norm(Xa_means - Xt, dim=1)   # ||xa - xt||\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(err_b.detach().cpu().numpy(), 'r--', label=\"||xb - xt||\")\n",
    "plt.plot(err_a.detach().cpu().numpy(), 'b-', label=\"||xa - xt||\")\n",
    "plt.xlabel(\"time step\")\n",
    "plt.ylabel(\"error norm\")\n",
    "plt.title(\"Error norms: background vs analysis\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"lorenz53_aipf_error.png\")\n",
    "plt.show()\n",
    "\n",
    "# ---- average errors (mean over time) ----\n",
    "avg_err_b = err_b.mean().item()\n",
    "avg_err_a = err_a.mean().item()\n",
    "print(f\"Average background error: {avg_err_b:.3f}\")\n",
    "print(f\"Average analysis error:   {avg_err_a:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef61c94-fd79-49bd-9dc4-492accde2ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (5) ENSEMBLE SCATTER: stacked subplots, each with its own legend, incl. xb/xa means\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# --- settings ---\n",
    "num_panels = 8\n",
    "plane = \"xy\"  # {\"xy\",\"xz\",\"yz\"}\n",
    "idx_map = {\"xy\": (0,1), \"xz\": (0,2), \"yz\": (1,2)}\n",
    "i, j = idx_map[plane]\n",
    "\n",
    "# pick evenly spaced assimilation indices (exclude 0)\n",
    "sample_idx = np.linspace(1, Nt-1, num_panels, dtype=int)\n",
    "sample_idx = np.unique(sample_idx)\n",
    "\n",
    "# --- capture ensembles ---\n",
    "device = X_truth.device\n",
    "dtype  = X_truth.dtype\n",
    "\n",
    "x0_bias = X_truth[0] + torch.tensor([2.0, -2.0, 3.0], device=device, dtype=dtype)\n",
    "Xa_prev = x0_bias.expand(Npart, 3) + 2.0 * torch.randn(Npart, 3, device=device, dtype=dtype)\n",
    "\n",
    "captured = []\n",
    "for n in range(Nt):\n",
    "    # Forecast\n",
    "    Xb = Xa_prev\n",
    "    h = dt_assim / nsub\n",
    "    for _ in range(nsub):\n",
    "        Xb = rk4_step(Xb, h, lorenz63_rhs, params_model)\n",
    "    if model_std > 0:\n",
    "        Xb = Xb + model_std * torch.randn_like(Xb)\n",
    "\n",
    "    y = Y[n]\n",
    "    Xa = net(Xb, y)\n",
    "\n",
    "    if n in sample_idx:\n",
    "        xt = X_truth[n]\n",
    "        err_b = torch.norm(Xb.mean(dim=0) - xt).item()\n",
    "        err_a = torch.norm(Xa.mean(dim=0) - xt).item()\n",
    "        captured.append({\n",
    "            \"n\": n,\n",
    "            \"xt\": xt.detach().cpu(),\n",
    "            \"Xb\": Xb.detach().cpu(),\n",
    "            \"Xa\": Xa.detach().cpu(),\n",
    "            \"err_b\": err_b,\n",
    "            \"err_a\": err_a,\n",
    "        })\n",
    "\n",
    "    Xa_prev = Xa.detach()\n",
    "\n",
    "# --- plotting & saving ---\n",
    "rows = len(captured)\n",
    "for snap in captured:\n",
    "    n = snap[\"n\"]\n",
    "    xt = snap[\"xt\"].numpy()\n",
    "    Xb = snap[\"Xb\"].numpy()\n",
    "    Xa = snap[\"Xa\"].numpy()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # ensemble members\n",
    "    ax.scatter(Xb[:, i], Xb[:, j], s=15, alpha=0.4, c=\"red\", label=\"forecast ens\")\n",
    "    ax.scatter(Xa[:, i], Xa[:, j], s=15, alpha=0.4, marker=\"^\", c=\"blue\", label=\"analysis ens\")\n",
    "    ax.scatter([xt[i]], [xt[j]], s=90, marker=\"*\", c=\"gold\", edgecolor=\"k\", label=\"truth\")\n",
    "\n",
    "    # means\n",
    "    mu_b = Xb.mean(axis=0); mu_a = Xa.mean(axis=0)\n",
    "    ax.scatter([mu_b[i]], [mu_b[j]], s=80, edgecolor=\"k\", facecolor=\"none\", label=\"xb mean\")\n",
    "    ax.scatter([mu_a[i]], [mu_a[j]], s=80, edgecolor=\"k\", facecolor=\"none\", marker=\"s\", label=\"xa mean\")\n",
    "\n",
    "    ax.set_title(f\"step n={n} | ||xb-xt||={snap['err_b']:.2f}, ||xa-xt||={snap['err_a']:.2f}\")\n",
    "    ax.set_xlabel(f\"x_{i+1}\")\n",
    "    ax.set_ylabel(f\"x_{j+1}\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    ax.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    fig.suptitle(f\"Ensemble scatter in $x_{{{i+1}}}$–$x_{{{j+1}}}$ plane\", fontsize=12)\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    # save figure with time step in filename\n",
    "    fname = f\"l63_ensemble_scatter_step_{n:04d}.png\"\n",
    "    fig.savefig(fname, dpi=150)\n",
    "    print(f\"Saved {fname}\")\n",
    "\n",
    "    plt.close(fig)  # close to avoid memory buildup\n",
    "\n",
    "# error table\n",
    "print(\"Step |  ||xb-xt||   ||xa-xt||\")\n",
    "for snap in captured:\n",
    "    print(f\"{snap['n']:4d} |  {snap['err_b']:9.3f}  {snap['err_a']:9.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbe838-9bdb-4d92-8a6d-e2d823b04856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
