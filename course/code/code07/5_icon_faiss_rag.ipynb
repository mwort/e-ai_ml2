{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb435bb-9baa-40e3-a60f-2a6d7cd3cb9c",
   "metadata": {},
   "source": [
    "# Notebook 3 â€” Semantic Search on the ICON Model Code Base\n",
    "\n",
    "In this notebook we build a **practical, reusable semantic search system**\n",
    "for the open-source ICON weather and climate model.\n",
    "\n",
    "We will:\n",
    "- clone the ICON Git repository,\n",
    "- extract and chunk relevant text and code files,\n",
    "- build a FAISS vector index,\n",
    "- and perform meaningful semantic queries.\n",
    "\n",
    "This notebook focuses on **clean structure, reproducibility, and usability**.\n",
    "Generated data (document copies, embeddings, FAISS indices) are **excluded from Git**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4d69c0-98fc-4c79-ba4b-ee4ab36cdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT : /Users/rpotthas/all/e-ai_ml2/course/code/code07\n",
      "DOCUMENTS_DIR: /Users/rpotthas/all/e-ai_ml2/course/code/code07/documents\n",
      "INDEX_DIR    : /Users/rpotthas/all/e-ai_ml2/course/code/code07/vector_db\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Project structure and paths\n",
    "# ------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Root of this notebook (assumed to live inside the course repo)\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "\n",
    "# Where external repositories are cloned\n",
    "DOCUMENTS_DIR = PROJECT_ROOT / \"documents\"\n",
    "\n",
    "# Where FAISS indices and metadata are stored\n",
    "INDEX_DIR = PROJECT_ROOT / \"vector_db\"\n",
    "\n",
    "# Create directories if they do not exist\n",
    "DOCUMENTS_DIR.mkdir(exist_ok=True)\n",
    "INDEX_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT :\", PROJECT_ROOT)\n",
    "print(\"DOCUMENTS_DIR:\", DOCUMENTS_DIR)\n",
    "print(\"INDEX_DIR    :\", INDEX_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151a820f-9862-478b-963e-b4891115c01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to .gitignore: documents/\n",
      "Added to .gitignore: vector_db/\n",
      "Added to .gitignore: *.index\n",
      "Added to .gitignore: *.faiss\n",
      "Added to .gitignore: __pycache__/\n",
      "Added to .gitignore: .ipynb_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Ensure generated folders are ignored by Git\n",
    "# ------------------------------------------------------------\n",
    "gitignore_path = PROJECT_ROOT / \".gitignore\"\n",
    "\n",
    "ignore_entries = [\n",
    "    \"documents/\",\n",
    "    \"vector_db/\",\n",
    "    \"*.index\",\n",
    "    \"*.faiss\",\n",
    "    \"__pycache__/\",\n",
    "    \".ipynb_checkpoints/\"\n",
    "]\n",
    "\n",
    "existing = set()\n",
    "if gitignore_path.exists():\n",
    "    existing = set(gitignore_path.read_text().splitlines())\n",
    "\n",
    "with gitignore_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    for entry in ignore_entries:\n",
    "        if entry not in existing:\n",
    "            f.write(entry + \"\\n\")\n",
    "            print(\"Added to .gitignore:\", entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b939f8-5382-44a0-abc2-77634d118760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy configured for HTTP/HTTPS:\n",
      "  http://ofsquid.dwd.de:8080\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Proxy configuration (DWD network)\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "PROXY = \"http://ofsquid.dwd.de:8080\"\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"]  = PROXY\n",
    "os.environ[\"HTTPS_PROXY\"] = PROXY\n",
    "os.environ[\"http_proxy\"]  = PROXY\n",
    "os.environ[\"https_proxy\"] = PROXY\n",
    "\n",
    "# Do not proxy local connections\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "print(\"Proxy configured for HTTP/HTTPS:\")\n",
    "print(\" \", PROXY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd878e25-b7ed-493f-8347-dfee44f27b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Klone nach '/Users/rpotthas/all/e-ai_ml2/course/code/code07/documents/icon-model'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICON repository cloned.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Clone ICON model repository (shallow clone)\n",
    "# ------------------------------------------------------------\n",
    "import subprocess\n",
    "\n",
    "ICON_REPO_URL = \"https://gitlab.dkrz.de/icon/icon-model.git\"\n",
    "ICON_DIR = DOCUMENTS_DIR / \"icon-model\"\n",
    "\n",
    "if not ICON_DIR.exists():\n",
    "    subprocess.run(\n",
    "        [\"git\", \"clone\", \"--depth\", \"1\", ICON_REPO_URL, str(ICON_DIR)],\n",
    "        check=True\n",
    "    )\n",
    "    print(\"ICON repository cloned.\")\n",
    "else:\n",
    "    print(\"ICON repository already exists:\", ICON_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e28032-7438-4581-aa2c-5dc95765d35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".clang-format\n",
      ".cmake-format.py\n",
      ".fprettify.rc\n",
      ".git\n",
      ".git-blame-ignore-revs\n",
      ".gitattributes\n",
      ".gitignore\n",
      ".gitlab\n",
      ".gitlab-ci.yml\n",
      ".gitmodules\n",
      ".pre-commit-config.yaml\n",
      "AUTHORS.TXT\n",
      "CONTRIBUTING.md\n",
      "LICENSES\n",
      "Makefile.in\n",
      "README.md\n",
      "RELEASE_NOTES.md\n",
      "REUSE.toml\n",
      "collect.extra-libs.in\n",
      "config\n",
      "config.h.in\n",
      "configure\n",
      "configure.ac\n",
      "data\n",
      "depgen.c.config.in\n",
      "depgen.f90.config.in\n",
      "deplist.config.in\n",
      "doc\n",
      "etc\n",
      "externals\n",
      "icon.mk.in\n",
      "inlib.mk.in\n",
      "m4\n",
      "make_runscripts\n",
      "ragnarok\n",
      "run\n",
      "schedulers\n",
      "scripts\n",
      "src\n",
      "support\n",
      "test\n",
      "utils\n",
      "vertical_coord_tables\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Sanity check: list top-level ICON repository content\n",
    "# ------------------------------------------------------------\n",
    "for p in sorted(ICON_DIR.iterdir()):\n",
    "    print(p.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67fc71d5-f8a8-4525-ba8f-02a170361c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# File selection rules for ICON repository\n",
    "# ------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "\n",
    "# Directories we explicitly skip\n",
    "SKIP_DIRS = {\n",
    "    \".git\",\n",
    "    \".gitlab\",\n",
    "    \"externals\",      # external dependencies\n",
    "    \"data\",           # often large, non-text\n",
    "    \"LICENSES\"\n",
    "}\n",
    "\n",
    "# File extensions we consider searchable text/code\n",
    "TEXT_EXTENSIONS = {\n",
    "    \".f90\", \".F90\", \".f\", \".F\",\n",
    "    \".c\", \".h\", \".cpp\", \".hpp\",\n",
    "    \".py\", \".sh\",\n",
    "    \".txt\", \".md\", \".rst\", \".org\",\n",
    "    \".cfg\", \".ini\", \".nml\",\n",
    "    \".yaml\", \".yml\", \".toml\",\n",
    "    \".mk\"\n",
    "}\n",
    "\n",
    "# Some important files have no extension\n",
    "ALLOW_EXTENSIONLESS = {\n",
    "    \"configure\",\n",
    "    \"make_runscripts\",\n",
    "    \"README\",\n",
    "    \"NOTICE\"\n",
    "}\n",
    "\n",
    "# Safety limit: skip very large files\n",
    "MAX_FILE_SIZE = 2_000_000  # 2 MB\n",
    "\n",
    "\n",
    "def is_searchable_file(path: Path) -> bool:\n",
    "    \"\"\"Decide whether a file should be indexed.\"\"\"\n",
    "    if not path.is_file():\n",
    "        return False\n",
    "\n",
    "    # Skip unwanted directories\n",
    "    if any(part in SKIP_DIRS for part in path.parts):\n",
    "        return False\n",
    "\n",
    "    # Skip large files\n",
    "    if path.stat().st_size > MAX_FILE_SIZE:\n",
    "        return False\n",
    "\n",
    "    # Extension-based check\n",
    "    if path.suffix in TEXT_EXTENSIONS:\n",
    "        return True\n",
    "\n",
    "    # Extensionless but known-important files\n",
    "    if path.suffix == \"\" and path.name in ALLOW_EXTENSIONLESS:\n",
    "        return True\n",
    "\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e794d4ca-8c84-47ab-9bcf-6209fbb09fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of searchable files: 1211\n",
      "\n",
      "REUSE.toml\n",
      "make_runscripts\n",
      "configure\n",
      ".pre-commit-config.yaml\n",
      "README.md\n",
      "CONTRIBUTING.md\n",
      "RELEASE_NOTES.md\n",
      ".gitlab-ci.yml\n",
      ".cmake-format.py\n",
      "etc/o3_cmip_provider.py\n",
      "etc/aero_provider.py\n",
      "utils/test_driver.py\n",
      "utils/icon_sorted_deps.sh\n",
      "utils/pvcs.py\n",
      "utils/move_to_prefix.sh\n",
      "support/mo_util_uuid.f90\n",
      "support/index_list.h\n",
      "support/util_multifile_restart.c\n",
      "support/mo_util_uuid_types.f90\n",
      "support/util_uuid.h\n",
      "support/util_uuid.c\n",
      "vertical_coord_tables/README\n",
      "ragnarok/CMakeLists.txt\n",
      "ragnarok/README.md\n",
      "run/create-hostfile.py\n",
      "run/ana_varnames_map_file.txt\n",
      "run/README\n",
      "run/run_wrapper.sh\n",
      "run/postFunx.sh\n",
      "test/unit-tests/test_start_mpi.mpi.f90\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Preview which files will be indexed\n",
    "# ------------------------------------------------------------\n",
    "searchable_files = []\n",
    "\n",
    "for p in ICON_DIR.rglob(\"*\"):\n",
    "    if is_searchable_file(p):\n",
    "        searchable_files.append(p)\n",
    "\n",
    "print(f\"Number of searchable files: {len(searchable_files)}\\n\")\n",
    "\n",
    "for p in searchable_files[:30]:\n",
    "    print(p.relative_to(ICON_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75bd15-264c-4439-9c40-2a9581a4b6c0",
   "metadata": {},
   "source": [
    "## Chunking strategy\n",
    "\n",
    "We do **not** embed whole files.\n",
    "\n",
    "Reasons:\n",
    "- Files are often too long for meaningful embeddings\n",
    "- Queries usually target *local* functionality\n",
    "- Chunking improves retrieval precision\n",
    "\n",
    "We therefore split files into **overlapping text chunks**:\n",
    "- small enough to be specific\n",
    "- large enough to preserve context\n",
    "\n",
    "Each chunk will later be stored as one vector in the FAISS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30752ab1-f2fc-41e6-8583-b74ed5a14c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Utilities: safe text reading and normalization\n",
    "# ------------------------------------------------------------\n",
    "import re\n",
    "\n",
    "def read_text_file(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Read a text file robustly.\n",
    "    Try UTF-8 first, fall back to latin-1 if needed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return path.read_text(encoding=\"latin-1\", errors=\"replace\")\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize whitespace and line endings.\n",
    "    Keeps line structure (important for code).\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # Collapse excessive empty lines\n",
    "    text = re.sub(r\"\\n{4,}\", \"\\n\\n\\n\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aee5da2-b751-42c0-b362-abe24b7b881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Chunking function (line-based, overlapping)\n",
    "# ------------------------------------------------------------\n",
    "from typing import List, Dict\n",
    "\n",
    "def chunk_text_by_lines(\n",
    "    text: str,\n",
    "    target_chars: int = 1800,\n",
    "    overlap_chars: int = 300\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks based on line structure.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Full text of a file.\n",
    "    target_chars : int\n",
    "        Approximate size of each chunk (in characters).\n",
    "    overlap_chars : int\n",
    "        Overlap between consecutive chunks (in characters).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of dicts with:\n",
    "        - 'text'\n",
    "        - 'line_start'\n",
    "        - 'line_end'\n",
    "    \"\"\"\n",
    "    lines = text.splitlines()\n",
    "    chunks = []\n",
    "\n",
    "    buffer = []\n",
    "    buffer_len = 0\n",
    "    start_line = 1\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        buffer.append(line)\n",
    "        buffer_len += len(line) + 1\n",
    "\n",
    "        if buffer_len >= target_chars:\n",
    "            chunk_text = \"\\n\".join(buffer).strip()\n",
    "            end_line = i + 1\n",
    "\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"line_start\": start_line,\n",
    "                \"line_end\": end_line,\n",
    "            })\n",
    "\n",
    "            # Prepare overlap\n",
    "            overlap_buffer = []\n",
    "            overlap_len = 0\n",
    "            for prev_line in reversed(buffer):\n",
    "                overlap_buffer.insert(0, prev_line)\n",
    "                overlap_len += len(prev_line) + 1\n",
    "                if overlap_len >= overlap_chars:\n",
    "                    break\n",
    "\n",
    "            buffer = overlap_buffer\n",
    "            buffer_len = overlap_len\n",
    "            start_line = end_line - len(buffer) + 1\n",
    "\n",
    "    # Remaining text\n",
    "    if buffer:\n",
    "        chunks.append({\n",
    "            \"text\": \"\\n\".join(buffer).strip(),\n",
    "            \"line_start\": start_line,\n",
    "            \"line_end\": len(lines),\n",
    "        })\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd2a0d9f-7473-4e16-90c4-6206811099c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 26883\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Build chunks from ICON repository\n",
    "# ------------------------------------------------------------\n",
    "chunks = []\n",
    "\n",
    "for path in searchable_files:\n",
    "    raw_text = read_text_file(path)\n",
    "    text = normalize_text(raw_text)\n",
    "\n",
    "    if len(text.strip()) < 100:\n",
    "        continue  # skip trivial files\n",
    "\n",
    "    file_chunks = chunk_text_by_lines(text)\n",
    "\n",
    "    for i, ch in enumerate(file_chunks):\n",
    "        chunks.append({\n",
    "            \"file\": str(path.relative_to(ICON_DIR)),\n",
    "            \"chunk_id\": i,\n",
    "            \"line_start\": ch[\"line_start\"],\n",
    "            \"line_end\": ch[\"line_end\"],\n",
    "            \"text\": ch[\"text\"],\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "589a9dbc-a05e-432a-9ecc-37904e87310c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "File: REUSE.toml\n",
      "Lines: 1â€“64\n",
      "--------------------------------------------------------------------------------\n",
      "# ICON\n",
      "#\n",
      "# ---------------------------------------------------------------\n",
      "# Copyright (C) 2004-2025, DWD, MPI-M, DKRZ, KIT, ETH, MeteoSwiss\n",
      "# Contact information: icon-model.org\n",
      "# See AUTHORS.TXT for a list of authors\n",
      "# See LICENSES/ for license information\n",
      "# SPDX-License-Identifier: CC0-1.0\n",
      "# ---------------------------------------------------------------\n",
      "\n",
      "version = 1\n",
      "\n",
      "[[annotations]]\n",
      "# Documentation and its source files:\n",
      "path = [\n",
      "  \".gitlab/ci/**/README.md\",\n",
      "  \"AUTHORS.TXT\",\n",
      "  \"CONTRIBUTING.md\",\n",
      "  \"README.md\",\n",
      "  \"RELEASE_NOTES.md\",\n",
      "  \"config/**/*.readme\",\n",
      "  \"config/**/README.md\",\n",
      "  \"data/README\",\n",
      "  \"doc/**/*.eps\",\n",
      "  \"doc/**/*.ico\",\n",
      "  \"doc/**/*.jpg\",\n",
      "  \"doc/**/*.md\",\n",
      "  \"doc/**/*.pdf\",\n",
      "  \"doc/**/*.png\",\n",
      "  \"doc/**/*.svg\",\n",
      "  \"ragnarok/README.md\",\n",
      "  \"run/**/README*\",\n",
      "  \"schedulers/ecmwf/doc/\n",
      "================================================================================\n",
      "File: REUSE.toml\n",
      "Lines: 58â€“108\n",
      "--------------------------------------------------------------------------------\n",
      "path = [\n",
      "  \".gitlab/ci/data/system-checksum-table.yml\",\n",
      "  \".gitlab/issue_templates/bug-report.md\",\n",
      "  \".gitlab/issue_templates/build-failure-report.md\",\n",
      "  \".gitlab/issue_templates/feature-request.md\",\n",
      "  \".gitlab/merge_request_templates/default.md\",\n",
      "  \".gitlab/merge_request_templates/nwp-feature.md\",\n",
      "  \"config.h.in\",\n",
      "  \"config/**/*_ENV_TAG\",\n",
      "  \"config/**/*_TAG*\",\n",
      "  \"run/tolerance/PROBTEST_TAG\",\n",
      "  \"run/tolerance/hashes/*\",\n",
      "  \"schedulers/ecmwf/gen/mapfile_vlist.txt\",\n",
      "  \"scripts/**/TAG\",\n",
      "  \"scripts/jenkins_scripts/scripts/test.euler.cpu\",\n",
      "  \"scripts/postprocessing/amip_quickplots/VARS.txt\",\n",
      "  \"scripts/postprocessing/tools/iconplot_ncl/*.list\",\n",
      "  \"test/microphysics_1mom_schemes/input-data.nc.md5\",\n",
      "]\n",
      "SPDX-FileCopyrightText = \"2004-2025, DWD, MPI-M, DKRZ, KIT, ETH, MeteoSwiss\"\n",
      "SPDX-License-Identif\n",
      "================================================================================\n",
      "File: REUSE.toml\n",
      "Lines: 99â€“127\n",
      "--------------------------------------------------------------------------------\n",
      "SPDX-FileCopyrightText = \"2015-2018, Atmospheric and Environmental Research and Regents of the University of Colorado. All rights reserved.\"\n",
      "SPDX-License-Identifier = \"BSD-3-Clause\"\n",
      "\n",
      "[[annotations]]\n",
      "path = \"utils/plotems/lightbox/images/*\"\n",
      "SPDX-FileCopyrightText = \"2005, 2014 jQuery Foundation, Inc. and other contributors\"\n",
      "SPDX-License-Identifier = \"MIT\"\n",
      "\n",
      "[[annotations]]\n",
      "path = \"configure\"\n",
      "SPDX-License-Identifier = \"FSFUL\"\n",
      "\n",
      "[[annotations]]\n",
      "path = [\"utils/config.guess\", \"utils/config.sub\"]\n",
      "SPDX-License-Identifier = \"GPL-3.0-or-later WITH Autoconf-exception-generic-3.0\"\n",
      "\n",
      "[[annotations]]\n",
      "path = \"utils/install-sh\"\n",
      "SPDX-License-Identifier = \"X11-distribute-modifications-variant\"\n",
      "\n",
      "[[annotations]]\n",
      "path = \"utils/log-plotting/bootstrap.min.css\"\n",
      "SPDX-FileCopyrightText = \"Copyright 2011-2021 The Boot\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Inspect a few chunks manually\n",
    "# ------------------------------------------------------------\n",
    "for c in chunks[:3]:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"File: {c['file']}\")\n",
    "    print(f\"Lines: {c['line_start']}â€“{c['line_end']}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(c[\"text\"][:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dac4527-c23c-4e50-a928-0f2d83035e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding model: all-MiniLM-L6-v2\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load sentence embedding model\n",
    "# ------------------------------------------------------------\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Compact, fast, good semantic quality\n",
    "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "print(\"Loaded embedding model:\", MODEL_NAME)\n",
    "print(\"Embedding dimension:\", model.get_sentence_embedding_dimension())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1f6c81c-0245-470a-b78c-070761468d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 26883\n",
      "Example text length: 1846\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Prepare chunk texts for embedding\n",
    "# ------------------------------------------------------------\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "\n",
    "print(\"Number of chunks:\", len(texts))\n",
    "print(\"Example text length:\", len(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53594cbd-6669-4cb0-872b-47ece55971fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f627757f484bcf8eb5d66ff1e49faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/841 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding array shape: (26883, 384)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Compute embeddings\n",
    "# ------------------------------------------------------------\n",
    "import numpy as np\n",
    "\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Ensure FAISS-compatible dtype\n",
    "embeddings = embeddings.astype(np.float32)\n",
    "\n",
    "print(\"Embedding array shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6654a737-0135-48b3-9bec-598f3de51f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding norms:\n",
      "  min: 0.9999999\n",
      "  max: 1.0000001\n",
      "  mean: 1.0\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Sanity checks\n",
    "# ------------------------------------------------------------\n",
    "assert embeddings.ndim == 2\n",
    "assert embeddings.shape[0] == len(chunks)\n",
    "assert embeddings.shape[1] == 384\n",
    "\n",
    "# Norm statistics (useful for cosine similarity)\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(\"Embedding norms:\")\n",
    "print(\"  min:\", norms.min())\n",
    "print(\"  max:\", norms.max())\n",
    "print(\"  mean:\", norms.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dd9bda3-7ae5-47b9-b2fc-74beeeea52e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: REUSE.toml\n",
      "Lines: 1 - 64\n",
      "--------------------------------------------------------------------------------\n",
      "# ICON\n",
      "#\n",
      "# ---------------------------------------------------------------\n",
      "# Copyright (C) 2004-2025, DWD, MPI-M, DKRZ, KIT, ETH, MeteoSwiss\n",
      "# Contact information: icon-model.org\n",
      "# See AUTHORS.TXT for a list of authors\n",
      "# See LICENSES/ for license information\n",
      "# SPDX-License-Identifier: CC0-1.0\n",
      "# ---------------------------------------------------------------\n",
      "\n",
      "version = 1\n",
      "\n",
      "[[annotations]]\n",
      "# Documentation and its source files:\n",
      "path = [\n",
      "  \".gitlab/ci/**/README.md\",\n",
      "  \"AUTHORS.TXT\",\n",
      "  \"CONTRIBUTING.md\",\n",
      "  \"README.md\",\n",
      "  \"RELEASE_NOTES.md\",\n",
      "  \"config/**/*.readme\",\n",
      "  \"config/**/README.md\",\n",
      "  \"data/RE\n",
      "--------------------------------------------------------------------------------\n",
      "Embedding (first 10 components):\n",
      "[-0.11404255  0.09105776 -0.05297947 -0.0285041   0.02929485 -0.03326541\n",
      "  0.03339936 -0.00376081  0.02134291 -0.01417605]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Inspect one embedded chunk\n",
    "# ------------------------------------------------------------\n",
    "i = 0\n",
    "\n",
    "print(\"File:\", chunks[i][\"file\"])\n",
    "print(\"Lines:\", chunks[i][\"line_start\"], \"-\", chunks[i][\"line_end\"])\n",
    "print(\"-\" * 80)\n",
    "print(chunks[i][\"text\"][:600])\n",
    "print(\"-\" * 80)\n",
    "print(\"Embedding (first 10 components):\")\n",
    "print(embeddings[i][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2f138-d1a0-4e8a-8410-5958bc3425f7",
   "metadata": {},
   "source": [
    "## Why FAISS?\n",
    "\n",
    "Brute-force similarity search compares a query vector against **all** stored vectors:\n",
    "- Cost: ð’ª(N Â· d) per query\n",
    "- Too slow for large libraries\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) provides:\n",
    "- Fast nearest-neighbor search\n",
    "- Optimized C++ backend\n",
    "- Exact or approximate algorithms\n",
    "- CPU and GPU support\n",
    "\n",
    "We start with the **simplest exact index**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "519bc672-31a9-4b23-b742-4d26041e5ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built\n",
      "Number of vectors: 26883\n",
      "Dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Build FAISS index (exact, cosine similarity)\n",
    "# ------------------------------------------------------------\n",
    "import faiss\n",
    "\n",
    "d = embeddings.shape[1]   # embedding dimension\n",
    "index = faiss.IndexFlatIP(d)  # inner product = cosine similarity (unit vectors)\n",
    "\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"FAISS index built\")\n",
    "print(\"Number of vectors:\", index.ntotal)\n",
    "print(\"Dimension:\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "850caf09-3c09-4c60-8312-4aef70785634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# FAISS query function\n",
    "# ------------------------------------------------------------\n",
    "def search_faiss(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Search the FAISS index for the most similar chunks.\n",
    "    Returns a list of (score, metadata).\n",
    "    \"\"\"\n",
    "    query_vec = model.encode(\n",
    "        query,\n",
    "        convert_to_numpy=True\n",
    "    ).astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "    scores, indices = index.search(query_vec, top_k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(scores[0], indices[0]):\n",
    "        meta = chunks[idx]\n",
    "        results.append((score, meta))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "683761bc-9e9f-40c1-9f99-ecfad681eca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Rank 1 | similarity = 0.631\n",
      "File: doc/www/atmosphere/sbm/sbm_overview.md\n",
      "Lines: 26 - 31\n",
      "--------------------------------------------------------------------------------\n",
      "ICON uses total water and ice contents (qc, qr, qi, qs, qg) in many places in the code. When using SBM, we keep advecting these total water contents, but overwrite them by the integrals over the corresponding size distributions, right after the call to the microphysics. The vertical turbulent diffusion \"turbdiff\" also uses cloud water, ice and snow (qc, qi, qs). For consistency, the corresponding SBM bins (tracers) were added to the vertical turbulent diffusion as well. Convection scheme might also detrain qc, qi, qr, qs. For mass conservation, we \"evaporate\" this water, transfering it to water wapor qv, and subtract the corresponding latent heat release. In appropreate conditions, this water vapor may condensate to hydrometeors as part of SBM. Total water contents are used in ICON initial\n",
      "================================================================================\n",
      "Rank 2 | similarity = 0.622\n",
      "File: doc/www/atmosphere/ecrad/ecrad_overview.md\n",
      "Lines: 234 - 246\n",
      "--------------------------------------------------------------------------------\n",
      "By default, ICON assumes that `FSD=1` everywhere, i.e. the normalised width of the condensate distribution is the same everywhere. However, observations show that this is not the case. Condensate is distributed more homogeneously in stratiform clouds compared to cumuliform clouds. Also, grid boxes containing cloud edges (i.e. not overcast, with a `cloud fraction < 1`) have wider condensate distributions than overcast grid boxes, because cloud edges naturally contain less condensate than the cloud interior.\n",
      "\n",
      "To account for this effect, a regime-dependent parameterization for the FSD parameter can be used by setting the namelist parameter in the `radiation_nml`:\n",
      "\n",
      "{term}`lcalculate_fsd`` = .true.`\n",
      "\n",
      "This parameterization is based on the publications {term}`Ahlgrimm et al. 2016` and {term}`Ahlg\n",
      "================================================================================\n",
      "Rank 3 | similarity = 0.615\n",
      "File: src/granules/microphysics_1mom_schemes/gscp_cloudice.f90\n",
      "Lines: 1 - 47\n",
      "--------------------------------------------------------------------------------\n",
      "! ICON\n",
      "!\n",
      "! ---------------------------------------------------------------\n",
      "! Copyright (C) 2004-2025, DWD, MPI-M, DKRZ, KIT, ETH, MeteoSwiss\n",
      "! Contact information: icon-model.org\n",
      "!\n",
      "! See AUTHORS.TXT for a list of authors\n",
      "! See LICENSES/ for license information\n",
      "! SPDX-License-Identifier: BSD-3-Clause\n",
      "! ---------------------------------------------------------------\n",
      "\n",
      "! Description of *gscp_cloudice*:\n",
      "!   This module procedure calculates the rates of change of temperature, cloud\n",
      "!   water, cloud ice, water vapor, rain and snow due to cloud microphysical\n",
      "!   processes related to the formation of grid scale precipitation. This\n",
      "!   includes the sedimentation of rain and snow. The precipitation fluxes at\n",
      "!   the surface are also calculated here.\n",
      "!\n",
      "! Method:\n",
      "!   Prognostic one-moment bulk microphy\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Example query\n",
    "# ------------------------------------------------------------\n",
    "query = \"How is cloud microphysics treated in ICON?\"\n",
    "\n",
    "results = search_faiss(query, top_k=3)\n",
    "\n",
    "for rank, (score, r) in enumerate(results, start=1):\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Rank {rank} | similarity = {score:.3f}\")\n",
    "    print(\"File:\", r[\"file\"])\n",
    "    print(\"Lines:\", r[\"line_start\"], \"-\", r[\"line_end\"])\n",
    "    print(\"-\" * 80)\n",
    "    print(r[\"text\"][:800])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aab492-6aa5-4daf-874e-39841f434d97",
   "metadata": {},
   "source": [
    "## Interpreting FAISS scores\n",
    "\n",
    "Because vectors are normalized:\n",
    "- Score â‰ˆ 1.0 â†’ very similar meaning\n",
    "- Score â‰ˆ 0.7 â†’ related topic\n",
    "- Score < 0.4 â†’ weak relation\n",
    "\n",
    "Exact values depend on:\n",
    "- chunk size\n",
    "- wording\n",
    "- abstraction level\n",
    "\n",
    "What matters is **relative ranking**, not absolute numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028f60e-78e8-4fb9-af86-76c12b3abd8c",
   "metadata": {},
   "source": [
    "## RAG step: Retrieve â†’ Generate\n",
    "\n",
    "RAG combines:\n",
    "1. **Retrieval**: find the most relevant ICON chunks via FAISS\n",
    "2. **Generation**: ask an LLM to answer using *only* the retrieved context\n",
    "\n",
    "Key goals:\n",
    "- answers grounded in the repository\n",
    "- traceable references (file + line ranges)\n",
    "- limited context size (avoid overflowing the model context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6651a007-4df0-4580-88e6-a25774b7af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Build a compact context from retrieved chunks\n",
    "# ------------------------------------------------------------\n",
    "def build_context(query: str, top_k: int = 8, max_chars: int = 12000):\n",
    "    \"\"\"\n",
    "    Retrieve top_k chunks and build a context string that fits into max_chars.\n",
    "    Returns (context_text, references_list).\n",
    "    \"\"\"\n",
    "    results = search_faiss(query, top_k=top_k)\n",
    "\n",
    "    context_parts = []\n",
    "    refs = []\n",
    "    total = 0\n",
    "\n",
    "    for score, r in results:\n",
    "        header = f\"[{r['file']} | lines {r['line_start']}-{r['line_end']} | sim={score:.3f}]\"\n",
    "        body = r[\"text\"].strip()\n",
    "\n",
    "        block = header + \"\\n\" + body\n",
    "        block_len = len(block) + 2\n",
    "\n",
    "        if total + block_len > max_chars:\n",
    "            break\n",
    "\n",
    "        context_parts.append(block)\n",
    "        refs.append((r[\"file\"], r[\"line_start\"], r[\"line_end\"], float(score)))\n",
    "        total += block_len\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    return context_text, refs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b0b768f-fb71-43c6-b22a-4c04912dc062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded.\n",
      "OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load API keys from .env file\n",
    "# ------------------------------------------------------------\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env in the current directory (or parent dirs)\n",
    "load_dotenv()\n",
    "\n",
    "# Sanity check (do NOT print the key itself)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found in environment\")\n",
    "\n",
    "print(\"OPENAI_API_KEY loaded.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI backend (requires OPENAI_API_KEY in environment)\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "USE_OPENAI = True  # set False if you want local-only\n",
    "\n",
    "if USE_OPENAI:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    print(\"OpenAI client initialized.\")\n",
    "else:\n",
    "    print(\"OpenAI disabled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf9a759c-2698-40e9-ad4a-06f7d9481c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# RAG: Retrieval + OpenAI generation\n",
    "# Streaming output with live Markdown display\n",
    "# ------------------------------------------------------------\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def rag_answer_openai_streaming(\n",
    "    query: str,\n",
    "    top_k: int = 10,\n",
    "    max_context_chars: int = 12000,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve relevant ICON chunks via FAISS and generate\n",
    "    a grounded Markdown answer using OpenAI with streaming output.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Retrieval ------------------------------------------------------------\n",
    "    results = search_faiss(query, top_k=top_k)\n",
    "\n",
    "    context_parts = []\n",
    "    total_chars = 0\n",
    "\n",
    "    for score, r in results:\n",
    "        header = (\n",
    "            f\"[{r['file']} | lines {r['line_start']}-{r['line_end']} \"\n",
    "            f\"| similarity={score:.3f}]\"\n",
    "        )\n",
    "        body = r[\"text\"].strip()\n",
    "\n",
    "        block = header + \"\\n\" + body\n",
    "        block_len = len(block) + 2\n",
    "\n",
    "        if total_chars + block_len > max_context_chars:\n",
    "            break\n",
    "\n",
    "        context_parts.append(block)\n",
    "        total_chars += block_len\n",
    "\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "\n",
    "    # --- Prompt ---------------------------------------------------------------\n",
    "    system_prompt = (\n",
    "        \"You are a technical assistant for the ICON weather model code base. \"\n",
    "        \"Answer strictly based on the provided context. \"\n",
    "        \"If the answer is not contained in the context, say so explicitly. \"\n",
    "        \"Write the answer in clear Markdown. \"\n",
    "        \"Include a final section titled 'Sources' listing the file names \"\n",
    "        \"and line ranges you used.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = (\n",
    "        f\"## Question\\n{query}\\n\\n\"\n",
    "        f\"## Context (ICON repository excerpts)\\n{context}\\n\\n\"\n",
    "        \"## Instructions\\n\"\n",
    "        \"- Give a concise, technical answer\\n\"\n",
    "        \"- Use bullet points where appropriate\\n\"\n",
    "        \"- Do not introduce information not present in the context\\n\"\n",
    "    )\n",
    "\n",
    "    # --- Streaming OpenAI call ------------------------------------------------\n",
    "    stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    accumulated_text = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        delta = chunk.choices[0].delta\n",
    "\n",
    "        if delta and delta.content:\n",
    "            accumulated_text += delta.content\n",
    "            display_handle.update(Markdown(accumulated_text))\n",
    "\n",
    "    return accumulated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5b30fb7-737d-441f-93c7-5030249b65d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The turbulence scheme in the ICON model is implemented in several modules, primarily located in the following files:\n",
       "\n",
       "- **Turbulence Utilities**:\n",
       "  - **File**: `src/atm_phy_schemes/turb_utilities.f90`\n",
       "  - **Key Routines**:\n",
       "    - `turb_setup`: Setting up the turbulence model.\n",
       "    - `init_basic_atmo_turb`: Initialization of basic properties of atmospheric turbulence.\n",
       "    - `solve_turb_budgets`: Solution of prognostic TKE-equation and related equations.\n",
       "\n",
       "- **Turbulent Mixing Interface**:\n",
       "  - **File**: `src/atm_phy_aes/mo_interface_aes_tmx.f90`\n",
       "  - **Functionality**: Calls the turbulent mixing scheme and surface schemes.\n",
       "\n",
       "- **Vertical Diffusion Configuration**:\n",
       "  - **File**: `src/configure_model/mo_turb_vdiff_config.f90`\n",
       "  - **Description**: Configuration structure for the VDIFF turbulence scheme.\n",
       "\n",
       "- **Vertical Diffusion Parameters**:\n",
       "  - **File**: `src/atm_phy_schemes/mo_turb_vdiff_params.f90`\n",
       "  - **Parameters**: Defines parameters for the VDIFF turbulence scheme.\n",
       "\n",
       "- **Smagorinsky Turbulent Mixing**:\n",
       "  - **File**: `src/atm_phy_aes/tmx/mo_tmx_smagorinsky.f90`\n",
       "  - **Functionality**: Contains classes and functions for the Smagorinsky turbulent mixing scheme.\n",
       "\n",
       "- **Numerics for Turbulent Mixing**:\n",
       "  - **File**: `src/atm_phy_aes/tmx/mo_tmx_numerics.f90`\n",
       "  - **Functionality**: Implements numerical methods for turbulent mixing processes.\n",
       "\n",
       "These modules collectively handle various aspects of turbulence modeling within the ICON framework.\n",
       "\n",
       "### Sources\n",
       "- `src/atm_phy_schemes/turb_utilities.f90 | lines 1-35`\n",
       "- `src/atm_phy_aes/mo_interface_aes_tmx.f90 | lines 1-48`\n",
       "- `src/configure_model/mo_turb_vdiff_config.f90 | lines 1-39`\n",
       "- `src/atm_phy_schemes/mo_turb_vdiff_params.f90 | lines 1-41`\n",
       "- `src/atm_phy_aes/tmx/mo_tmx_smagorinsky.f90 | lines 1-58`\n",
       "- `src/atm_phy_aes/tmx/mo_tmx_numerics.f90 | lines 1-53`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Where is the turbulence scheme implemented in ICON?\"\n",
    "answer_md = rag_answer_openai_streaming(query, top_k=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e774f4-644b-4718-9bef-fadeeffeaf72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
