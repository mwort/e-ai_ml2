{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34eaa6a8-bc03-4acf-acdd-ad085f18c54f",
   "metadata": {},
   "source": [
    "# Direct OpenAI Queries (Yet without Retrieval)\n",
    "\n",
    "In this notebook we interact **directly** with an OpenAI Large Language Model (LLM).\n",
    "\n",
    "This represents the **baseline usage pattern**:\n",
    "- no external documents\n",
    "- no vector databases\n",
    "- no Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Understanding this baseline is essential before introducing more advanced architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4132e17-1de1-4870-9e8b-469e0a994610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy configured:\n",
      "  http://ofsquid.dwd.de:8080\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Proxy configuration (DWD network)\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "PROXY = \"http://ofsquid.dwd.de:8080\"\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"]  = PROXY\n",
    "os.environ[\"HTTPS_PROXY\"] = PROXY\n",
    "os.environ[\"http_proxy\"]  = PROXY\n",
    "os.environ[\"https_proxy\"] = PROXY\n",
    "\n",
    "# Do not proxy local traffic\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "print(\"Proxy configured:\")\n",
    "print(\" \", PROXY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b73b99-8bb7-42bd-aa55-8c95592e24e3",
   "metadata": {},
   "source": [
    "## Secure API key handling\n",
    "\n",
    "API keys must **never** be hard-coded into notebooks or scripts.\n",
    "\n",
    "We load them from a `.env` file, which:\n",
    "- is ignored by Git\n",
    "- keeps credentials local\n",
    "- supports reproducible workflows\n",
    "\n",
    "This is standard practice in research and production systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ffd179f-2bb9-452a-9a8e-d02c49566018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded.\n",
      "OpenAI client initialized.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Load API keys from .env file\n",
    "# ------------------------------------------------------------\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env in the current directory (or parent dirs)\n",
    "load_dotenv()\n",
    "\n",
    "# Sanity check (do NOT print the key itself)\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found in environment\")\n",
    "\n",
    "print(\"OPENAI_API_KEY loaded.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# OpenAI backend (requires OPENAI_API_KEY in environment)\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "\n",
    "USE_OPENAI = True  # set False if you want local-only\n",
    "\n",
    "if USE_OPENAI:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    print(\"OpenAI client initialized.\")\n",
    "else:\n",
    "    print(\"OpenAI disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff7480-9ed5-4df2-a663-0731907b08c1",
   "metadata": {},
   "source": [
    "## First minimal query\n",
    "\n",
    "We now send a simple prompt to the model and receive a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1874425-7df7-487c-9ecd-a4c61a4ab2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A transformer model is a type of neural network architecture designed for processing sequential data, particularly in natural language processing tasks. It uses self-attention mechanisms to weigh the importance of different words in a sequence, allowing the model to capture long-range dependencies and contextual relationships effectively. Transformers have become the foundation for many advanced language models, including BERT and GPT, due to their scalability and performance in generating and understanding text.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain what a transformer model is in three sentences.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037a924-24af-49f0-a3b6-daad5b4880eb",
   "metadata": {},
   "source": [
    "## Structured prompting\n",
    "\n",
    "Using system and user messages allows better control over tone and scope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83210949-bed7-4d67-8a86-cd93250d13ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention is a mechanism used in neural networks, particularly in transformer architectures, to compute a representation of a sequence by relating different positions within that sequence. Here’s a mathematical breakdown of self-attention:\n",
       "\n",
       "1. **Input Representation**: \n",
       "   Let \\( X \\in \\mathbb{R}^{n \\times d} \\) be the input matrix, where \\( n \\) is the number of tokens (or words) in the sequence, and \\( d \\) is the dimensionality of each token's embedding.\n",
       "\n",
       "2. **Linear Transformations**:\n",
       "   We create three matrices: Query \\( Q \\), Key \\( K \\), and Value \\( V \\) by applying learned linear transformations:\n",
       "   \\[\n",
       "   Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
       "   \\]\n",
       "   where \\( W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k} \\) are weight matrices for queries, keys, and values, respectively, and \\( d_k \\) is the dimensionality of the keys.\n",
       "\n",
       "3. **Dot Product Attention Scores**:\n",
       "   The attention scores are computed using the dot product of the queries and keys:\n",
       "   \\[\n",
       "   \\text{Attention\\_scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
       "   \\]\n",
       "   The division by \\( \\sqrt{d_k} \\) is a scaling factor to prevent the dot products from growing too large, which can lead to gradients that are too small.\n",
       "\n",
       "4. **Softmax Normalization**:\n",
       "   We apply the softmax function to the attention scores to obtain the attention weights:\n",
       "   \\[\n",
       "   \\text{Attention\\_weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
       "   \\]\n",
       "   This results in a matrix where each row sums to 1, representing the distribution of attention for each token.\n",
       "\n",
       "5. **Weighted Sum of Values**:\n",
       "   The output of the self-attention mechanism is computed as a weighted sum of the values:\n",
       "   \\[\n",
       "   \\text{Output} = \\text{Attention\\_weights} \\cdot V\n",
       "   \\]\n",
       "\n",
       "6. **Final Output**:\n",
       "   The output matrix is of the same shape as the input matrix \\( X \\), allowing it to be used in subsequent layers of the model.\n",
       "\n",
       "In summary, self-attention allows each token to attend to all other tokens in the sequence, capturing contextual relationships effectively. The entire process can be summarized as:\n",
       "\\[\n",
       "\\text{Output} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
       "\\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise technical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain self-attention mathematically.\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5f550-4e21-4c80-b3fd-52c904679987",
   "metadata": {},
   "source": [
    "## Streaming model responses\n",
    "\n",
    "By default, language model responses are returned only after the full answer\n",
    "has been generated. This is simple, but it can feel slow for longer outputs.\n",
    "\n",
    "With **streaming**, the model sends partial outputs token by token.\n",
    "This allows:\n",
    "- immediate feedback to the user\n",
    "- progressive rendering of long answers\n",
    "- more interactive user interfaces\n",
    "\n",
    "Streaming does **not** change the model’s reasoning or final content —\n",
    "it only affects how the output is delivered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196d7290-12cf-44e3-9195-f4f33c412de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention is a mechanism used in neural networks, particularly in transformer architectures, to compute a representation of a sequence by relating different positions within that sequence. Here's a mathematical explanation of self-attention:\n",
       "\n",
       "1. **Input Representation**: \n",
       "   Let \\( X \\in \\mathbb{R}^{n \\times d} \\) be the input matrix, where \\( n \\) is the number of tokens in the sequence and \\( d \\) is the dimensionality of each token's embedding.\n",
       "\n",
       "2. **Linear Transformations**:\n",
       "   We create three matrices: the Query \\( Q \\), Key \\( K \\), and Value \\( V \\) matrices by applying learned linear transformations:\n",
       "   \\[\n",
       "   Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
       "   \\]\n",
       "   where \\( W^Q, W^K, W^V \\in \\mathbb{R}^{d \\times d_k} \\) are weight matrices for queries, keys, and values, respectively, and \\( d_k \\) is the dimensionality of the queries and keys.\n",
       "\n",
       "3. **Attention Scores**:\n",
       "   The attention scores are computed by taking the dot product of the query with all keys:\n",
       "   \\[\n",
       "   \\text{scores} = QK^T \\in \\mathbb{R}^{n \\times n}\n",
       "   \\]\n",
       "   This results in a matrix where each element \\( \\text{scores}_{ij} \\) represents the attention score of the \\( i \\)-th token with respect to the \\( j \\)-th token.\n",
       "\n",
       "4. **Scaling**:\n",
       "   To prevent large values in the softmax function, the scores are scaled by the square root of the dimensionality of the keys:\n",
       "   \\[\n",
       "   \\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}}\n",
       "   \\]\n",
       "\n",
       "5. **Softmax**:\n",
       "   We apply the softmax function to the scaled scores to obtain the attention weights:\n",
       "   \\[\n",
       "   \\text{attention\\_weights} = \\text{softmax}(\\text{scaled\\_scores}) \\in \\mathbb{R}^{n \\times n}\n",
       "   \\]\n",
       "   Each row of this matrix sums to 1 and represents the distribution of attention for each token.\n",
       "\n",
       "6. **Weighted Sum of Values**:\n",
       "   Finally, the output of the self-attention mechanism is computed as a weighted sum of the values:\n",
       "   \\[\n",
       "   \\text{output} = \\text{attention\\_weights} V \\in \\mathbb{R}^{n \\times d_v}\n",
       "   \\]\n",
       "   where \\( d_v \\) is the dimensionality of the values (often equal to \\( d \\)).\n",
       "\n",
       "In summary, self-attention allows each token to attend to all other tokens in the sequence, producing a context-aware representation that captures dependencies regardless of their distance in the input sequence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise technical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain self-attention mathematically.\"}\n",
    "    ],\n",
    "    temperature=0.2,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "accumulated = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    delta = chunk.choices[0].delta\n",
    "    if delta and delta.content:\n",
    "        accumulated += delta.content\n",
    "        display_handle.update(Markdown(accumulated))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e90bb-d569-4848-bfab-744eab36e44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
