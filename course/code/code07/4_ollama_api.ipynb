{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c78cec5b-89c5-4992-b2f4-00a0f108b24c",
   "metadata": {},
   "source": [
    "## Querying local LLMs with Ollama\n",
    "\n",
    "Ollama allows running large language models **locally** on your machine.\n",
    "This removes any dependency on external APIs and keeps all data on-site.\n",
    "\n",
    "In this notebook we use Ollama for:\n",
    "- direct local queries\n",
    "- streaming responses\n",
    "- comparison with cloud-based models\n",
    "\n",
    "We use `llama3` as the default model, but the same code works for\n",
    "`mistral` or `deepseek-r1` if they are installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57405e3f-6c58-44e1-aebe-987a18dc053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: llama3\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Select Ollama model\n",
    "# ------------------------------------------------------------\n",
    "OLLAMA_MODEL = \"llama3\"   # alternatives: \"mistral\", \"deepseek-r1\"\n",
    "\n",
    "print(\"Using Ollama model:\", OLLAMA_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9217a49-c2e9-4bc2-ab59-94dcb37cc72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention is a key component of transformer models, and it's often misunderstood due to its complex mathematical formulation. Here's a step-by-step breakdown:\n",
       "\n",
       "**Notations:**\n",
       "\n",
       "* `Q`, `K`, and `V` are matrices representing queries, keys, and values, respectively.\n",
       "* `n` is the sequence length (number of tokens in the input).\n",
       "* `h` is the hidden size (dimensionality of the embeddings).\n",
       "\n",
       "**Self-Attention Layer:**\n",
       "\n",
       "1. **Query-Key Matrix Multiplication**: Compute the dot product between `Q` and `K` to obtain a matrix `A`:\n",
       "\n",
       "$$A = \\frac{Q K^T}{\\sqrt{h}}$$\n",
       "\n",
       "where `^T` denotes the transpose.\n",
       "\n",
       "2. **Softmax Function**: Apply the softmax function to `A` element-wise, which normalizes the attention weights:\n",
       "\n",
       "$$Attention(A) = softmax(A) = \\frac{\\exp(a_i)}{\\sum_j \\exp(a_j)}$$\n",
       "\n",
       "where `a_i` is the `i`-th element of `A`.\n",
       "\n",
       "3. **Weighted Sum**: Compute the weighted sum of the values matrix `V` using the attention weights:\n",
       "\n",
       "$$Output = Concat(head_1, ..., head_k) W^O$$\n",
       "\n",
       "where `head_i = Attention(A) V_i`, and `W^O` is a learnable weight matrix.\n",
       "\n",
       "**Mathematical Representation:**\n",
       "\n",
       "The self-attention layer can be represented mathematically as follows:\n",
       "\n",
       "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{h}}) \\cdot V$$\n",
       "\n",
       "This representation captures the attention mechanism's ability to weigh and combine the input values based on their similarity.\n",
       "\n",
       "By breaking down this process into smaller components, you can better understand how self-attention works."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = ollama.chat(\n",
    "    model=OLLAMA_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise technical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain self-attention mathematically.\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "answer = response[\"message\"][\"content\"]\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08597949-32db-42ba-9b66-c648e35502be",
   "metadata": {},
   "source": [
    "## Streaming responses with Ollama\n",
    "\n",
    "Ollama also supports streaming output.\n",
    "As with OpenAI, streaming delivers tokens incrementally and improves\n",
    "interactivity for longer answers.\n",
    "\n",
    "Again, streaming changes *how* results are delivered, not *what*\n",
    "the model ultimately produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02251a19-e6a6-4ec5-afe2-abaf8f588870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Self-attention is a key component in Transformer models, which allows the model to weigh the importance of different input elements relative to each other.\n",
       "\n",
       "Mathematically, self-attention can be represented as:\n",
       "\n",
       "1. Query, Key, and Value matrices:\n",
       "\t* Q: Query matrix (shape: `(batch_size, sequence_length, embedding_dim)`), where `embedding_dim` is the dimensionality of the embeddings.\n",
       "\t* K: Key matrix (same shape as Q).\n",
       "\t* V: Value matrix (same shape as Q).\n",
       "\n",
       "2. Dot Product Attention:\n",
       "\t* Compute attention scores for each input element using the dot product between query and key matrices:\n",
       "\n",
       "`attention_scores = softmax(Q * K^T / sqrt(d))`\n",
       "\n",
       "where `d` is the embedding dimension, and `softmax` is applied along the last axis (i.e., sequence length).\n",
       "\n",
       "3. Weighted Sum:\n",
       "\t* Compute weighted sum of values based on attention scores:\n",
       "\n",
       "`output = V * attention_scores`\n",
       "\n",
       "The output is a matrix with shape `(batch_size, sequence_length, embedding_dim)`, where each row corresponds to an input element and its corresponding attention weights.\n",
       "\n",
       "Note that the self-attention mechanism can be scaled by a learnable parameter `Î±` (alpha), which helps to regulate the overall magnitude of the attention weights. This can be incorporated into the computation as follows:\n",
       "\n",
       "`output = V * alpha * attention_scores`\n",
       "\n",
       "In practice, you would implement this using libraries like TensorFlow or PyTorch, where you can use built-in functions for matrix multiplications and softmax operations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=OLLAMA_MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise technical assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain self-attention mathematically.\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "accumulated = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "        accumulated += chunk[\"message\"][\"content\"]\n",
    "        display_handle.update(Markdown(accumulated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb95d1-ebc6-406a-807a-90b47ad1042b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
