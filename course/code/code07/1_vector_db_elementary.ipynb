{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43603207-00a8-4000-ac0e-9954365ddc8a",
   "metadata": {},
   "source": [
    "# Notebook 1 — Elementary Vector Database Example\n",
    "\n",
    "This notebook introduces the **core idea behind vector databases** in a minimal and intuitive way.\n",
    "\n",
    "We start with a **small set of simple sentences**, convert them into numerical vectors using a pretrained language model, and then perform **semantic similarity search**.  \n",
    "No files, no PDFs, no large code bases — just meaning mapped into a geometric space.\n",
    "\n",
    "### Learning goals\n",
    "- Understand how text is mapped into a vector space\n",
    "- See how semantic similarity emerges from vector geometry\n",
    "- Perform a simple similarity search without keywords\n",
    "\n",
    "This notebook focuses on **intuition and concepts**.  \n",
    "In the next notebook, we will apply the same ideas to **real documents and PDFs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e2509d8-252e-4d2d-abc1-4533cca1abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 10\n",
      " 0: The sun is shining brightly today.\n",
      " 1: It is a sunny day with clear skies.\n",
      " 2: Heavy rain is falling over the city.\n",
      " 3: The weather forecast predicts strong rainfall.\n",
      " 4: Neural networks can learn complex patterns from data.\n",
      " 5: Machine learning models improve through training.\n",
      " 6: The cat is sleeping on the sofa.\n",
      " 7: A dog is running quickly in the park.\n",
      " 8: Transformers use self-attention mechanisms.\n",
      " 9: Large language models are based on transformer architectures.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 1: Define a small set of example sentences\n",
    "# ------------------------------------------------------------\n",
    "# These sentences are intentionally simple and cover\n",
    "# different semantic topics. Some are paraphrases, others\n",
    "# are clearly unrelated. This makes similarity effects visible.\n",
    "\n",
    "sentences = [\n",
    "    \"The sun is shining brightly today.\",\n",
    "    \"It is a sunny day with clear skies.\",\n",
    "    \"Heavy rain is falling over the city.\",\n",
    "    \"The weather forecast predicts strong rainfall.\",\n",
    "    \"Neural networks can learn complex patterns from data.\",\n",
    "    \"Machine learning models improve through training.\",\n",
    "    \"The cat is sleeping on the sofa.\",\n",
    "    \"A dog is running quickly in the park.\",\n",
    "    \"Transformers use self-attention mechanisms.\",\n",
    "    \"Large language models are based on transformer architectures.\"\n",
    "]\n",
    "\n",
    "print(f\"Number of sentences: {len(sentences)}\")\n",
    "for i, s in enumerate(sentences):\n",
    "    print(f\"{i:2d}: {s}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "099d4a33-8321-4ffa-8cc4-cbe3b29eb377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DWD proxy configured: http://ofsquid.dwd.de:8080\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Proxy configuration for DWD (ofsquid)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "PROXY = \"http://ofsquid.dwd.de:8080\"\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"]  = PROXY\n",
    "os.environ[\"HTTPS_PROXY\"] = PROXY\n",
    "os.environ[\"http_proxy\"]  = PROXY\n",
    "os.environ[\"https_proxy\"] = PROXY\n",
    "\n",
    "# Do not proxy local connections\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "print(\"DWD proxy configured:\", PROXY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f10f59fe-08d1-4a93-929f-7d1a1cf047cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 2: Load a sentence embedding model\n",
    "# ------------------------------------------------------------\n",
    "# We use a small and fast sentence transformer model.\n",
    "# It maps each sentence to a fixed-size vector in R^d.\n",
    "# No training is needed here; we only use the pretrained model.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load model (downloads once, then cached locally)\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "print(\"Embedding dimension:\", model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f95309b-be13-45b8-a04b-095f171c9335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding array shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 3: Encode sentences into embedding vectors\n",
    "# ------------------------------------------------------------\n",
    "# Each sentence is mapped to a vector z in R^d.\n",
    "# The result is a matrix of shape (num_sentences, embedding_dim).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "embeddings = model.encode(sentences, convert_to_numpy=True)\n",
    "\n",
    "print(\"Embedding array shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd074ae1-7325-4182-bec9-3d6f95255fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First embedding vector (truncated):\n",
      "[ 0.033  0.134  0.104  0.072  0.037 -0.041  0.095 -0.063 -0.017 -0.02 ]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 4: Inspect embedding values (sanity check)\n",
    "# ------------------------------------------------------------\n",
    "# We do NOT interpret individual numbers.\n",
    "# The important point is: each sentence is now a point in space.\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(\"First embedding vector (truncated):\")\n",
    "print(embeddings[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4686be74-356d-4ac1-a57a-e487392f16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Today is bright and sunny.\n",
      "- The forecast expects a lot of rain.\n",
      "- How do neural networks learn from data?\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 5: Define example query sentences\n",
    "# ------------------------------------------------------------\n",
    "# These queries are paraphrases or conceptual variations\n",
    "# of some of the original sentences.\n",
    "\n",
    "queries = [\n",
    "    \"Today is bright and sunny.\",\n",
    "    \"The forecast expects a lot of rain.\",\n",
    "    \"How do neural networks learn from data?\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(\"-\", q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db701230-096a-4ad7-8860-bb586c449240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding shape: (3, 384)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 6: Encode queries into vectors\n",
    "# ------------------------------------------------------------\n",
    "# Queries are embedded in exactly the same vector space\n",
    "# as the original sentences.\n",
    "\n",
    "query_embeddings = model.encode(queries, convert_to_numpy=True)\n",
    "\n",
    "print(\"Query embedding shape:\", query_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d31b38-e5f2-423f-a4ac-9149eacc6187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 7: Define cosine similarity function\n",
    "# ------------------------------------------------------------\n",
    "# Cosine similarity measures the angle between vectors.\n",
    "# Values close to 1 mean high semantic similarity.\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed7b299-0596-4575-93e7-94fed60fd470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 8: Perform similarity search\n",
    "# ------------------------------------------------------------\n",
    "# For a given query vector, compute similarity to all\n",
    "# sentence embeddings and return the top-k matches.\n",
    "\n",
    "def search_similar_sentences(query_embedding, sentences, embeddings, top_k=3):\n",
    "    scores = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity(query_embedding, emb)\n",
    "        scores.append((i, score))\n",
    "    \n",
    "    # Sort by similarity (highest first)\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scores[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1dfa0d-944f-40fc-8223-7ba730edc044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: Today is bright and sunny.\n",
      "------------------------------------------------------------\n",
      "Score:  0.882 | Sentence: The sun is shining brightly today.\n",
      "Score:  0.767 | Sentence: It is a sunny day with clear skies.\n",
      "Score:  0.328 | Sentence: The weather forecast predicts strong rainfall.\n",
      "Score:  0.211 | Sentence: Heavy rain is falling over the city.\n",
      "\n",
      "============================================================\n",
      "Query: The forecast expects a lot of rain.\n",
      "------------------------------------------------------------\n",
      "Score:  0.786 | Sentence: The weather forecast predicts strong rainfall.\n",
      "Score:  0.553 | Sentence: Heavy rain is falling over the city.\n",
      "Score:  0.420 | Sentence: It is a sunny day with clear skies.\n",
      "Score:  0.260 | Sentence: The sun is shining brightly today.\n",
      "\n",
      "============================================================\n",
      "Query: How do neural networks learn from data?\n",
      "------------------------------------------------------------\n",
      "Score:  0.769 | Sentence: Neural networks can learn complex patterns from data.\n",
      "Score:  0.531 | Sentence: Machine learning models improve through training.\n",
      "Score:  0.232 | Sentence: Transformers use self-attention mechanisms.\n",
      "Score:  0.231 | Sentence: Large language models are based on transformer architectures.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 9: Run similarity search for each query\n",
    "# ------------------------------------------------------------\n",
    "# This is the core \"aha\" moment of the notebook.\n",
    "\n",
    "for q, q_emb in zip(queries, query_embeddings):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Query:\", q)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = search_similar_sentences(q_emb, sentences, embeddings, top_k=4)\n",
    "    \n",
    "    for idx, score in results:\n",
    "        print(f\"Score: {score:6.3f} | Sentence: {sentences[idx]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4bf0c-28b1-4cea-a2d6-a0d78d1c9633",
   "metadata": {},
   "source": [
    "### What we observe\n",
    "\n",
    "- Semantically related sentences are retrieved even if\n",
    "  they share no exact words.\n",
    "- Paraphrases are detected reliably.\n",
    "- Unrelated topics have much lower similarity scores.\n",
    "\n",
    "This demonstrates the core idea behind vector databases:\n",
    "**meaning is represented geometrically**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e0f14-22c1-47cd-8e29-b2e53f35d764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
