{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cabd3f79-0e42-4777-a1a5-5e24a228d72a",
   "metadata": {},
   "source": [
    "# Function Calling Basics (Local LLMs)\n",
    "\n",
    "This notebook demonstrates the **core idea of function calling** using a **local large language model** (LLaMA 3 via Ollama), *without* any agent framework or API-level support.\n",
    "\n",
    "The goal is to understand function calling in its most basic form:\n",
    "\n",
    "- The **LLM does not execute code**\n",
    "- The **LLM decides whether an action is needed**\n",
    "- The **LLM emits a structured request** (JSON)\n",
    "- The **system executes the function**\n",
    "\n",
    "This approach reflects how early agent systems (including DAWID, 2024) worked:\n",
    "by **constraining model output** and **parsing structured JSON**.\n",
    "\n",
    "Later notebooks will show how modern APIs and frameworks make this more robust.\n",
    "\n",
    "---\n",
    "\n",
    "## Demo setup\n",
    "\n",
    "We define a single conceptual function:\n",
    "\n",
    "**`get_time(timezone: string)`**\n",
    "\n",
    "If the user asks for the current time, the model should return a JSON object\n",
    "requesting a call to this function — and *nothing else*.\n",
    "\n",
    "This is *manual* function calling, implemented entirely through prompting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb10c8c3-1f56-4b96-9f9b-bcfec094c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the following command in your shell:\n",
      "\n",
      "ollama run llama3:latest << 'EOF'\n",
      "You are an AI assistant.\n",
      "\n",
      "You may respond in ONE of two ways only:\n",
      "\n",
      "1. Normal text\n",
      "2. A JSON object of the following exact form:\n",
      "\n",
      "{\n",
      "  \"tool_call\": {\n",
      "    \"name\": \"<function name>\",\n",
      "    \"arguments\": { ... }\n",
      "  }\n",
      "}\n",
      "\n",
      "Available function:\n",
      "- get_time(timezone: string)\n",
      "\n",
      "Rules:\n",
      "- If the user asks for the current time, you MUST return ONLY the JSON tool call.\n",
      "- Do NOT include explanations.\n",
      "- Do NOT include markdown.\n",
      "- Do NOT include any text outside the JSON object.\n",
      "- The JSON must be valid and parseable.\n",
      "\n",
      "User request:\n",
      "What time is it in UTC?\n",
      "EOF\n"
     ]
    }
   ],
   "source": [
    "# This cell demonstrates function calling with a local LLaMA 3 model via Ollama.\n",
    "# The model is instructed to return a JSON \"tool call\" instead of free text.\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are an AI assistant.\n",
    "\n",
    "You may respond in ONE of two ways only:\n",
    "\n",
    "1. Normal text\n",
    "2. A JSON object of the following exact form:\n",
    "\n",
    "{\n",
    "  \"tool_call\": {\n",
    "    \"name\": \"<function name>\",\n",
    "    \"arguments\": { ... }\n",
    "  }\n",
    "}\n",
    "\n",
    "Available function:\n",
    "- get_time(timezone: string)\n",
    "\n",
    "Rules:\n",
    "- If the user asks for the current time, you MUST return ONLY the JSON tool call.\n",
    "- Do NOT include explanations.\n",
    "- Do NOT include markdown.\n",
    "- Do NOT include any text outside the JSON object.\n",
    "- The JSON must be valid and parseable.\n",
    "\n",
    "User request:\n",
    "What time is it in UTC?\n",
    "\"\"\"\n",
    "\n",
    "print(\"Run the following command in your shell:\\n\")\n",
    "print(\"ollama run llama3:latest << 'EOF'\")\n",
    "print(prompt.strip())\n",
    "print(\"EOF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f3a27-a45b-4b94-b69f-d0612eebffbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05dacfbc-f45c-44b8-aa2d-8ccf6ed4431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output:\n",
      "\n",
      "{\n",
      "  \"tool_call\": {\n",
      "    \"name\": \"get_time\",\n",
      "    \"arguments\": {\"timezone\": \"UTC\"}\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Call a local LLaMA 3 model via Ollama from inside Jupyter,\n",
    "and capture a JSON-style function call.\n",
    "\n",
    "This demonstrates:\n",
    "- local LLM execution\n",
    "- constrained output\n",
    "- machine-readable tool requests\n",
    "\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "prompt = textwrap.dedent(\"\"\"\n",
    "You are an AI assistant.\n",
    "\n",
    "You may respond in ONE of two ways only:\n",
    "\n",
    "1. Normal text\n",
    "2. A JSON object of the following exact form:\n",
    "\n",
    "{\n",
    "  \"tool_call\": {\n",
    "    \"name\": \"<function name>\",\n",
    "    \"arguments\": { ... }\n",
    "  }\n",
    "}\n",
    "\n",
    "Available function:\n",
    "- get_time(timezone: string)\n",
    "\n",
    "Rules:\n",
    "- If the user asks for the current time, you MUST return ONLY the JSON tool call.\n",
    "- Do NOT include explanations.\n",
    "- Do NOT include markdown.\n",
    "- Do NOT include any text outside the JSON object.\n",
    "- The JSON must be valid and parseable.\n",
    "\n",
    "User request:\n",
    "What time is it in UTC?\n",
    "\"\"\")\n",
    "\n",
    "# Call Ollama\n",
    "result = subprocess.run(\n",
    "    [\"ollama\", \"run\", \"llama3:latest\"],\n",
    "    input=prompt,\n",
    "    text=True,\n",
    "    capture_output=True,\n",
    "    check=True,\n",
    ")\n",
    "\n",
    "raw_output = result.stdout.strip()\n",
    "\n",
    "print(\"Raw model output:\\n\")\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a52a67-169f-4326-abb8-6f09aa1081dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed tool call:\n",
      "Tool name: get_time\n",
      "Arguments: {'timezone': 'UTC'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parse the JSON function call emitted by the model.\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    tool_call = json.loads(raw_output)[\"tool_call\"]\n",
    "    print(\"Parsed tool call:\")\n",
    "    print(\"Tool name:\", tool_call[\"name\"])\n",
    "    print(\"Arguments:\", tool_call[\"arguments\"])\n",
    "except Exception as e:\n",
    "    print(\"❌ Failed to parse JSON output\")\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb63f17-6f22-4e28-a1d2-de097036d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Execute the requested function.\n",
    "Fixes name shadowing between string arguments and datetime.timezone.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "def get_time(timezone: str) -> str:\n",
    "    \"\"\"\n",
    "    Return the current time in the given timezone.\n",
    "    Supported values: UTC, CET\n",
    "    \"\"\"\n",
    "    tz = timezone.upper()\n",
    "\n",
    "    if tz == \"UTC\":\n",
    "        return datetime.now(pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "    if tz == \"CET\":\n",
    "        return datetime.now(pytz.timezone(\"Europe/Berlin\")).strftime(\n",
    "            \"%Y-%m-%d %H:%M:%S CET\"\n",
    "        )\n",
    "\n",
    "    return f\"Unsupported timezone: {timezone}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "712ce793-2ee8-4960-8428-a356071b5d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Proxy bypass set for localhost\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# DWD proxy\n",
    "os.environ[\"HTTP_PROXY\"]  = \"http://ofsquid.dwd.de:8080\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://ofsquid.dwd.de:8080\"\n",
    "\n",
    "# Optional but recommended\n",
    "os.environ[\"http_proxy\"]  = os.environ[\"HTTP_PROXY\"]\n",
    "os.environ[\"https_proxy\"] = os.environ[\"HTTPS_PROXY\"]\n",
    "\n",
    "import os\n",
    "\n",
    "# Explicitly bypass proxy for local Ollama\n",
    "os.environ[\"NO_PROXY\"] = \"localhost,127.0.0.1\"\n",
    "os.environ[\"no_proxy\"] = \"localhost,127.0.0.1\"\n",
    "\n",
    "print(\"✅ Proxy bypass set for localhost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238ef597-718b-4aa1-ac59-9faee11bb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function calling via Ollama REST API.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- REST-based LLM calls\n",
    "- JSON tool-call extraction\n",
    "- system-side function execution\n",
    "- feeding results back to the model\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime, timezone as dt_timezone\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace12d9b-69dc-45b7-b0e6-a68da0e68b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone as dt_timezone\n",
    "\n",
    "\"\"\"\n",
    "System tool implementation.\n",
    "The LLM never executes this.\n",
    "\"\"\"\n",
    "\n",
    "def get_time(timezone: str) -> str:\n",
    "    tz = timezone.upper()\n",
    "\n",
    "    if tz == \"UTC\":\n",
    "        return datetime.now(dt_timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S UTC\")\n",
    "\n",
    "    elif tz == \"CET\":\n",
    "        # Local time; assumes system is CET/CEST aware\n",
    "        return datetime.now().astimezone().strftime(\"%Y-%m-%d %H:%M:%S CET\")\n",
    "\n",
    "    else:\n",
    "        return f\"Unsupported timezone: {timezone}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831d5529-1ce4-468f-ad3c-407504f8f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prompt that forces JSON-only tool calls.\n",
    "\"\"\"\n",
    "\n",
    "TOOL_PROMPT = \"\"\"\n",
    "You are an AI assistant.\n",
    "\n",
    "You may respond in ONE of two ways only:\n",
    "\n",
    "1. Normal text\n",
    "2. A JSON object of the following exact form:\n",
    "\n",
    "{\n",
    "  \"name\": \"get_time\",\n",
    "  \"arguments\": {\n",
    "    \"timezone\": \"UTC\"\n",
    "  }\n",
    "}\n",
    "\n",
    "Available function:\n",
    "- get_time(timezone: string)\n",
    "\n",
    "Rules:\n",
    "- If the user asks for the current time, return ONLY the JSON tool call.\n",
    "- No explanations.\n",
    "- No markdown.\n",
    "- No additional text.\n",
    "- Output must be valid JSON.\n",
    "\n",
    "User request:\n",
    "What time is it in UTC?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e85404c1-464b-43d5-89f3-17410cf1277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw model output:\n",
      "\n",
      "{\n",
      "  \"name\": \"get_time\",\n",
      "  \"arguments\": {\n",
      "    \"timezone\": \"UTC\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Call Ollama via REST API.\n",
    "\"\"\"\n",
    "\n",
    "def ollama_call(prompt: str, model=\"llama3:latest\") -> str:\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": 0},\n",
    "        },\n",
    "        timeout=60,\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()[\"response\"].strip()\n",
    "\n",
    "\n",
    "raw_output = ollama_call(TOOL_PROMPT)\n",
    "\n",
    "print(\"Raw model output:\\n\")\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b92f4704-e163-4126-b9db-95aa0a169117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed tool call:\n",
      "Tool: get_time\n",
      "Arguments: {'timezone': 'UTC'}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Parse the JSON tool call emitted by the model.\n",
    "\"\"\"\n",
    "\n",
    "tool_call = json.loads(raw_output)\n",
    "\n",
    "tool_name = tool_call[\"name\"]\n",
    "tool_args = tool_call[\"arguments\"]\n",
    "\n",
    "print(\"Parsed tool call:\")\n",
    "print(\"Tool:\", tool_name)\n",
    "print(\"Arguments:\", tool_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9eefa1e-a6ff-4113-8737-4ebd25b14557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style='color:gray; font-style:italic'>Tool result: 2025-12-31 14:41:40 UTC</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Execute the tool and display the result in grey.\n",
    "\"\"\"\n",
    "\n",
    "if tool_name == \"get_time\":\n",
    "    # tool_args is expected to be {\"timezone\": \"...\"}\n",
    "    tool_result = get_time(**tool_args)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown tool: {tool_name}\")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        \"<div style='color:gray; font-style:italic'>\"\n",
    "        f\"Tool result: {tool_result}\"\n",
    "        \"</div>\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66178a7-d01d-4522-904f-a5071c9feb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The current time in UTC is December 31, 2025, at 2:41 PM."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feed the tool result back to the LLM\n",
    "and ask for a final user-facing answer.\n",
    "\"\"\"\n",
    "\n",
    "FOLLOWUP_PROMPT = f\"\"\"\n",
    "The following tool was executed:\n",
    "\n",
    "Function: get_time\n",
    "Arguments: {tool_args}\n",
    "Result: {tool_result}\n",
    "\n",
    "Now produce a short, clear answer for the user.\n",
    "\"\"\"\n",
    "\n",
    "final_answer = ollama_call(FOLLOWUP_PROMPT)\n",
    "\n",
    "display(Markdown(final_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b95d3-84fb-430b-9501-071ec7b591a5",
   "metadata": {},
   "source": [
    "# Lets define a function now to do all this in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64779360-f9ba-4ca5-8a4c-b497f7440cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tool_prompt(query: str) -> str:\n",
    "    return f\"\"\"\n",
    "You can call the following function:\n",
    "\n",
    "Function name: get_time\n",
    "Arguments (JSON):\n",
    "  - timezone_name (string)\n",
    "\n",
    "If the user request requires calling the function,\n",
    "respond ONLY with valid JSON like this:\n",
    "\n",
    "{{\n",
    "  \"name\": \"get_time\",\n",
    "  \"arguments\": {{\n",
    "    \"timezone_name\": \"UTC\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "If no function is needed, answer normally.\n",
    "\n",
    "User request:\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64766e6b-fb25-4eaf-bd77-8c0c2f2a4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def ai(query: str) -> str:\n",
    "    # ------------------------------------------------------------\n",
    "    # Step 1: Ask model for a tool call\n",
    "    # ------------------------------------------------------------\n",
    "    prompt = build_tool_prompt(query)\n",
    "    raw = ollama_call(prompt)\n",
    "\n",
    "    print(\"\\n--- Raw model output ---\")\n",
    "    print(raw)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Step 2: Extract JSON tool call (if present)\n",
    "    # ------------------------------------------------------------\n",
    "    match = re.search(r\"\\{.*\\}\", raw, re.DOTALL)\n",
    "    if not match:\n",
    "        # No tool call → model answered directly\n",
    "        return raw.strip()\n",
    "\n",
    "    tool_call = json.loads(match.group(0))\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Step 3: Execute tool locally\n",
    "    # ------------------------------------------------------------\n",
    "    if tool_call.get(\"name\") == \"get_time\":\n",
    "        args = tool_call.get(\"arguments\", {})\n",
    "\n",
    "        # Normalize argument names\n",
    "        if \"timezone_name\" in args:\n",
    "            args[\"timezone\"] = args.pop(\"timezone_name\")\n",
    "\n",
    "        result = get_time(**args)\n",
    "    else:\n",
    "        result = f\"Unknown tool: {tool_call.get('name')}\"\n",
    "\n",
    "    print(\"\\n--- Tool result (executed) ---\")\n",
    "    print(result)\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Step 4: Feed tool result back to the model\n",
    "    # ------------------------------------------------------------\n",
    "    followup_prompt = f\"\"\"\n",
    "The user asked:\n",
    "{query}\n",
    "\n",
    "You requested a tool call and received this result:\n",
    "{result}\n",
    "\n",
    "Provide a clear and concise answer to the user.\n",
    "\"\"\"\n",
    "\n",
    "    final = ollama_call(followup_prompt)\n",
    "\n",
    "    return final.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "314fb392-73aa-4076-9248-e0602878f512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw model output ---\n",
      "{\n",
      "  \"name\": \"get_time\",\n",
      "  \"arguments\": {\n",
      "    \"timezone_name\": \"CET\"\n",
      "  }\n",
      "}\n",
      "\n",
      "--- Tool result (executed) ---\n",
      "2025-12-31 15:41:42 CET\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'According to my information, the current time in Central European Time (CET) is December 31st, 15:41:42.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai(\"What is the current time in CET?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478c8aa-46f0-4fe1-bf51-75a741e97abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
